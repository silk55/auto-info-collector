## Updated on 2025.03.05
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#agent>agent</a></li>
    <li><a href=#llm>llm</a></li>
  </ol>
</details>

## agent

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-03-04**|**MPO: Boosting LLM Agents with Meta Plan Optimization**|Weimin Xiong et.al.|[2503.02682](http://arxiv.org/abs/2503.02682)|null|近期大语言模型（LLM）的进展使基于大语言模型的智能体能够成功处理交互式规划任务。然而，尽管取得了这些成功，现有方法往往存在规划幻觉问题，并且每个新智能体都需要重新训练。为应对这些挑战，我们提出了元规划优化（MPO）框架，该框架通过直接纳入明确指导来增强智能体的规划能力。与以往依赖复杂知识（这要么需要大量人力，要么缺乏质量保证）的方法不同，MPO通过元规划利用高级通用指导来辅助智能体规划，并能根据智能体任务执行的反馈对元规划进行持续优化。我们在两个具有代表性的任务上进行的实验表明，MPO显著优于现有的基线方法。此外，我们的分析表明，MPO提供了一种即插即用的解决方案，可提高任务完成效率，并增强在先前未见场景中的泛化能力。 |
|**2025-03-04**|**Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent**|Xingzuo Li et.al.|[2503.02519](http://arxiv.org/abs/2503.02519)|null|大语言模型（LLM）智能体通常采用逐步推理框架，在该框架中，它们将思考和行动过程交织在一起以完成给定任务。然而，这种范式存在一个根深蒂固的单次通过问题，即无论生成的每个中间思考是否正确，都会将其纳入推理轨迹，这可能导致不可逆转的错误传播。为解决这一问题，本文提出了一种名为生成器 - 辅助器逐步回滚（GA - 回滚）的新颖框架，以引导大语言模型智能体做出更好的决策。具体而言，GA - 回滚利用一个生成器与环境进行交互，并使用一个辅助器检查生成器产生的每个动作，当辅助器检测到错误动作时会触发回滚操作。此外，我们还针对回滚场景引入了两种额外的策略，以进一步提高其有效性。大量实验表明，在三个广泛使用的基准测试中，GA - 回滚相较于多个强大的基线方法取得了显著改进。我们的分析进一步表明，GA - 回滚可以作为一个强大的即插即用模块，与其他方法无缝集成。 |
|**2025-03-04**|**AppAgentX: Evolving GUI Agents as Proficient Smartphone Users**|Wenjia Jiang et.al.|[2503.02268](http://arxiv.org/abs/2503.02268)|null|近年来，大语言模型（LLM）的发展催生了能够与图形用户界面（GUI）进行交互的智能基于大语言模型的代理。这些代理展现出强大的推理和适应能力，使其能够执行传统上需要预定义规则的复杂任务。然而，基于大语言模型的代理依赖逐步推理，这往往导致效率低下，尤其是在处理常规任务时。相比之下，传统的基于规则的系统在效率方面表现出色，但缺乏应对新场景的智能和灵活性。为应对这一挑战，我们提出了一种用于图形用户界面代理的新型进化框架，该框架在保持智能和灵活性的同时提高了操作效率。我们的方法引入了一种记忆机制，用于记录代理的任务执行历史。通过分析这些历史记录，代理能够识别重复的动作序列，并进化出高级动作作为捷径，取代这些低级操作，从而提高效率。这使得代理能够专注于需要更复杂推理的任务，同时简化常规动作。在多个基准任务上的实验结果表明，我们的方法在效率和准确性方面均显著优于现有方法。代码将开源以支持进一步的研究。 |
|**2025-03-04**|**Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient and Feasible Multitasking with Time Constraints Between Actions**|Zirui Wu et.al.|[2503.02238](http://arxiv.org/abs/2503.02238)|null|尽管基于大语言模型的智能体在任务完成方面取得了显著进展，但现有的评估基准往往过度强调单任务性能，而对现实场景中至关重要的多任务规划和执行效率关注不足。为了弥补这一差距，我们提出了Recipe2Plan，这是一个基于现实烹饪场景的新型基准框架。与传统基准不同，Recipe2Plan要求智能体在遵守时间约束的前提下，通过并行任务执行来优化烹饪时间，即特定的操作需要在遵循前序步骤的特定时间间隔内完成。过度激进的局部并行化可能会破坏这一约束，从而可能影响整个烹饪过程。操作之间严格的时间约束给智能体带来了独特的挑战，即需要在最大化并发操作和遵守关键时间约束之间取得平衡。对最先进模型的大量实验表明，在效率和可行性之间保持这种平衡存在挑战。研究结果凸显了大语言模型需要提高时间感知能力和全局多任务处理能力。我们在https://github.com/WilliamZR/Recipe2Plan上开源了我们的基准和代码。 |
|**2025-03-04**|**ATLaS: Agent Tuning via Learning Critical Steps**|Zhixun Chen et.al.|[2503.02197](http://arxiv.org/abs/2503.02197)|null|大语言模型（LLM）智能体在多领域任务中展现出了卓越的泛化能力。现有的智能体调优方法通常在完整的专家轨迹上进行有监督微调。然而，对完整轨迹进行行为克隆会引入专家偏差，并削弱对专家数据未覆盖状态的泛化能力。此外，规划、对中间子任务进行复杂推理以及战略决策等关键步骤对于智能体任务的成功至关重要，因此学习这些步骤是提升大语言模型智能体的关键。为了实现更有效、高效的智能体调优，我们提出了 ATLaS 方法，该方法能够识别专家轨迹中的关键步骤，并仅在这些步骤上对大语言模型进行微调，从而降低成本。通过将训练重点转向少数关键步骤，我们的方法降低了对完整轨迹过拟合的风险，并促进了在不同环境和任务中的泛化能力。在大量实验中，仅使用 ATLaS 选出的 30% 关键步骤进行微调的大语言模型，其性能优于在所有步骤上进行微调的大语言模型以及近期的开源大语言模型智能体。ATLaS 能够保持并提升基础大语言模型作为与多样化环境交互的通用智能体的能力。 |
|**2025-03-03**|**Persuasion at Play: Understanding Misinformation Dynamics in Demographic-Aware Human-LLM Interactions**|Angana Borah et.al.|[2503.02038](http://arxiv.org/abs/2503.02038)|null|在错误信息暴露和易感性方面，现有的挑战因不同的人口群体而异，因为某些人群比其他人群更容易受到错误信息的影响。大语言模型（LLMs）凭借其大规模生成有说服力的内容并强化现有偏见的能力，为这些挑战带来了新的维度。本研究探讨了在接触错误信息时，大语言模型与人类之间的双向说服动态。我们利用人类立场数据集分析人类对大语言模型的影响，并通过生成基于大语言模型的有说服力的论点来评估大语言模型对人类的影响。此外，我们使用多智能体大语言模型框架来分析在说服作用下，面向不同人口特征的大语言模型智能体之间错误信息的传播情况。我们的研究结果表明，人口因素会影响大语言模型对错误信息的易感性，这与人类易感性中基于人口特征的模式密切相关。我们还发现，与人类人口群体类似，多智能体大语言模型也表现出回音室效应。本研究探索了人类与大语言模型之间的相互作用，强调了在错误信息背景下的人口差异，并为未来的干预措施提供了见解。 |
|**2025-03-03**|**MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents**|Kunlun Zhu et.al.|[2503.01935](http://arxiv.org/abs/2503.01935)|null|大型语言模型（LLMs）作为自主智能体展现出了卓越的能力，然而现有的基准测试要么聚焦于单智能体任务，要么局限于狭窄的领域，无法体现多智能体协作与竞争的动态过程。在本文中，我们推出了MultiAgentBench这一全面的基准测试，旨在评估基于大语言模型的多智能体系统在多样化、交互式场景中的表现。我们的框架不仅衡量任务完成情况，还使用新颖的、基于里程碑的关键绩效指标来评估协作与竞争的质量。此外，我们评估了各种协调协议（包括星形、链式、树形和图状拓扑结构）以及诸如小组讨论和认知规划等创新策略。值得注意的是，gpt - 4o - mini达到了平均最高任务得分，在研究场景中，图状结构在协调协议中表现最佳，认知规划使里程碑达成率提高了3%。代码和数据集可在https://github.com/MultiagentBench/MARBLE上公开获取。 |
|**2025-03-03**|**Can (A)I Change Your Mind?**|Miriam Havin et.al.|[2503.01844](http://arxiv.org/abs/2503.01844)|null|基于大语言模型（LLM）的对话代理日益融入日常生活，这引发了关于其影响人类观点的潜在能力的关键认知和社会问题。尽管先前的研究表明，基于大语言模型的代理能够生成有说服力的内容，但这些研究通常是在可控的英语环境中进行的。为解决这一问题，我们进行了预先注册的研究，探索大语言模型在更贴近现实、无约束的场景中的说服能力，同时考察静态（书面段落）和动态（通过 Telegram 进行对话）两种交互类型。该研究完全使用希伯来语进行，共有 200 名参与者，评估了大语言模型和人类对话者对有争议的公民政策话题的说服效果。结果显示，参与者对大语言模型和人类的观点采纳情况相似，无论对话者类型或交互模式如何，在所有条件下都出现了显著的观点变化。除了静态的大语言模型交互外，在大多数场景中，参与者的信心水平显著提高。这些发现表明，基于大语言模型的代理在不同来源和场景中都具有强大的说服能力，凸显了它们在塑造公众舆论方面的潜在影响。 |
|**2025-03-03**|**Student engagement in collaborative learning with AI agents in an LLM-empowered learning environment: A cluster analysis**|Zhanxin Hao et.al.|[2503.01694](http://arxiv.org/abs/2503.01694)|null|将大语言模型（LLM）集成到教育实践中，能够通过适应不同学习者类型的多样化行为模式来促进个性化学习。本研究旨在探索一种新型互动环境中的学习者类型，详细分析其独特特征和互动动态。研究选取了中国一所大学的110名学生，让他们在一个由大语言模型赋能的学习环境中与多个大语言模型智能体进行互动，并完成六个模块的课程学习。研究收集并分析了学生的非认知特质、课程参与度和与人工智能的互动模式等数据。通过层次聚类分析，学生被分为三个不同的群体：积极提问者、主动回应的导航者和沉默倾听者。随后，运用认知网络分析进一步描绘不同类型学习者的互动特征和认知参与度。研究结果强调了不同类型的学习者如何参与人机互动学习，并为自适应教育系统的设计提供了实际启示。 |
|**2025-03-02**|**Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity**|Yupu Hao et.al.|[2503.00771](http://arxiv.org/abs/2503.00771)|null|在涉及多种工具的交互场景中，个性化工具使用对于使大语言模型（LLMs）与用户偏好相契合至关重要。然而，当前大多数基准测试主要侧重于文本生成的个性化或直接的工具使用，而没有同时兼顾两者。在这项工作中，我们引入了一个新颖的基准测试ETAPP，用于评估个性化工具调用，建立了一个沙盒环境，并构建了一个包含800个测试用例的综合数据集，涵盖了多样化的用户画像。为了提高评估的准确性，我们提出了一种基于关键点的大语言模型评估方法，通过为每个测试用例手动标注关键点并将其作为参考提供给大语言模型，来减轻以大语言模型作为评判系统时的偏差。此外，我们对优秀的大语言模型进行了评估并提供了深入分析。我们还研究了不同工具调用策略对大语言模型个性化性能的影响以及微调在我们任务中的效果。我们还验证了偏好设置和基于关键点的评估方法的有效性。我们的研究结果为改进个性化大语言模型智能体提供了见解。我们的代码可在https://github.com/hypasd-art/ETAPP获取。 |

<p align=right>(<a href=#updated-on-20250305>back to top</a>)</p>

## llm

|Publish Date|Title|Authors|PDF|Code|abstract|
|---|---|---|---|---|---|
|**2025-03-04**|**Wikipedia in the Era of LLMs: Evolution and Risks**|Siming Huang et.al.|[2503.02879](http://arxiv.org/abs/2503.02879)|null|在本文中，我们对大语言模型（LLMs）对维基百科的影响进行了全面分析，通过现有数据考察维基百科的演变，并利用模拟研究潜在风险。我们首先分析页面浏览量和文章内容，以研究维基百科的近期变化并评估大语言模型的影响。随后，我们评估大语言模型如何影响与维基百科相关的各种自然语言处理（NLP）任务，包括机器翻译和检索增强生成（RAG）。我们的研究结果和模拟结果显示，维基百科文章受到了大语言模型的影响，在某些类别中的影响约为1% - 2%。如果基于维基百科的机器翻译基准受到大语言模型的影响，模型的得分可能会虚高，模型之间的比较结果也可能会发生变化。此外，如果知识库被大语言模型生成的内容污染，检索增强生成的有效性可能会降低。虽然大语言模型尚未完全改变维基百科的语言和知识结构，但我们认为，我们的实证研究结果表明有必要谨慎考虑未来可能出现的风险。 |
|**2025-03-04**|**The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models**|Ke Ji et.al.|[2503.02875](http://arxiv.org/abs/2503.02875)|null|提高大语言模型（LLM）的推理能力通常需要使用标注数据进行有监督微调或进行计算成本高昂的采样。我们提出了无监督前缀微调（UPFT）方法，该方法利用了前缀自一致性这一观察结果，即不同解决方案路径间共享的初始推理步骤，以提高大语言模型的推理效率。通过仅对初始前缀子串（少至 8 个词元）进行训练，UPFT 无需标注数据或进行详尽采样。在推理基准测试上的实验表明，UPFT 的性能与拒绝采样微调等有监督方法相当，同时将训练时间减少了 75%，采样成本降低了 99%。进一步分析显示，错误往往出现在推理过程的后期阶段，并且基于前缀的训练能够保留模型的结构知识。这项工作展示了最小限度的无监督微调如何能为大语言模型带来显著的推理能力提升，为传统方法提供了一种可扩展且资源高效的替代方案。 |
|**2025-03-04**|**Prompting Generative AI with Interaction-Augmented Instructions**|Leixian Shen et.al.|[2503.02874](http://arxiv.org/abs/2503.02874)|null|生成式人工智能（GenAI）模型，包括大语言模型和文本到图像模型的出现，凭借其卓越的能力，更重要的是通过文本提示这种直观的交流方式，极大地推动了人类与人工智能的协同发展。尽管文本指令直观易懂，但基于文本的指令存在自然语言固有的模糊性和冗余性问题。为解决这一问题，研究人员探索了通过交互来增强基于文本的指令，以促进更精确、有效地表达人类意图，例如直接操作。然而，交互增强指令的设计策略缺乏系统研究，这阻碍了我们对其的理解和应用。为全面呈现交互增强指令的情况，我们提出了一个框架，从为何、何时、何人、何事以及如何应用交互来增强基于文本的指令等方面对相关工具进行分析。值得注意的是，我们确定了应用交互的四个目的，包括限制、扩展、组织和细化文本指令。我们还总结了每个目的的设计范式，以造福未来的研究人员和从业者。 |
|**2025-03-04**|**FairSense-AI: Responsible AI Meets Sustainability**|Shaina Raza et.al.|[2503.02865](http://arxiv.org/abs/2503.02865)|null|在本文中，我们介绍了FairSense - AI：一个多模态框架，旨在检测和缓解文本与图像中的偏差。通过利用大语言模型（LLMs）和视觉 - 语言模型（VLMs），FairSense - AI能够发现内容中可能出现的微妙偏见或刻板印象，为用户提供偏差分数、解释性高亮显示以及关于公平性改进的自动建议。此外，FairSense - AI集成了一个人工智能风险评估组件，该组件与麻省理工学院人工智能风险库和美国国家标准与技术研究院（NIST）人工智能风险管理框架等框架相契合，能够对伦理和安全问题进行结构化识别。该平台通过模型剪枝和混合精度计算等技术实现了能源效率优化，从而减少了其对环境的影响。通过一系列案例研究和应用，我们展示了FairSense - AI如何通过解决公平性的社会层面问题以及大规模人工智能部署中对可持续性的迫切需求，来促进负责任的人工智能使用。https://vectorinstitute.github.io/FairSense - AI，https://pypi.org/project/fair - sense - ai/ |
|**2025-03-04**|**Calibrating LLM Confidence with Semantic Steering: A Multi-Prompt Aggregation Framework**|Ziang Zhou et.al.|[2503.02863](http://arxiv.org/abs/2503.02863)|null|大型语言模型（LLM）的置信度分数往往存在偏差，通常会高估其预测的可靠性。虽然大型语言模型的言语化置信度已受到关注，但先前的研究对于是否可以通过提示来系统地引导置信度分数仍存在分歧。近期研究甚至认为，这种由提示引起的置信度变化微不足道，表明大型语言模型的置信度校准对语言干预具有刚性。与这些观点相反，我们首先通过在7个基准测试中对三个模型（包括GPT3.5、LLAMA3 - 70b、GPT4）进行探测，严格证实了定向置信度变化的存在，证明了明确的指令可以有规律地提高或降低置信度分数。基于这一观察，我们提出了一个包含三个组件的新颖框架：置信度引导、引导后的置信度聚合和引导后答案选择，名为SteeringConf。我们的方法SteeringConf利用置信度操纵机制，将大型语言模型的置信度分数朝着多个期望的方向引导，随后通过一个汇总模块对引导后的置信度分数进行聚合，以得出最终预测。我们在7个基准测试中对该方法进行了评估，在置信度校准和失败检测任务中，该方法在校准指标方面始终优于基线方法。 |
|**2025-03-04**|**Privacy and Accuracy-Aware AI/ML Model Deduplication**|Hong Guan et.al.|[2503.02862](http://arxiv.org/abs/2503.02862)|null|随着差分隐私随机梯度下降（DP - SGD）等隐私保护机器学习算法的日益普及，在私有数据集上训练或微调模型变得越来越普遍。这一转变使得需要提供不同隐私保证和效用水平的模型，以满足用户的多样化需求。然而，管理大量不同版本的大型模型会带来重大的运营挑战，包括推理延迟增加、资源消耗增多和成本上升。模型去重是许多模型服务和数据库系统广泛采用的一种技术，用于支持高性能、低成本的推理查询和模型诊断查询。但现有的模型去重工作均未考虑隐私问题，这导致某些去重模型的隐私成本无限制累积，并且在对差分隐私训练的模型进行去重时效率低下。我们首次对差分隐私训练模型的去重问题进行了形式化定义，并提出了一种新颖的兼顾隐私和准确性的去重机制来解决这些问题。我们开发了一种贪心策略，用于选择基础模型并将其分配给目标模型，以最小化存储和隐私成本。在对目标模型进行去重时，我们动态安排准确性验证，并应用稀疏向量技术来降低与私有验证数据相关的隐私成本。与不提供隐私保证的基线方法相比，我们的方法使单个模型（包括大语言模型和视觉变换器）的压缩比最高提高了 35 倍。此外，由于减少了 I/O 操作，推理速度最高提升了 43 倍。 |
|**2025-03-04**|**Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers**|Zicong He et.al.|[2503.02851](http://arxiv.org/abs/2503.02851)|null|大型语言模型（LLMs）会产生幻觉，这一现象常与创造力相关联。此前的研究主要从理论或定性的角度探讨这种关联，而我们的工作采用定量方法，系统地研究大型语言模型中幻觉与创造力之间的关系。鉴于创造力的复杂性，我们提出了一个专门针对大型语言模型的狭义定义，并引入了一个评估框架HCL，该框架可量化大型语言模型在解码过程中不同层的幻觉和创造力。我们的实证分析揭示了幻觉与创造力之间存在一种权衡关系，这种关系在层深度、模型类型和模型大小方面具有一致性。值得注意的是，在不同的模型架构中，我们发现每个模型大小都存在一个特定的层，能最优地平衡这种权衡。此外，较大模型的最优层往往出现在早期层，且该层模型的置信度也显著更高。这些发现提供了一个定量视角，为理解大型语言模型的创造力和幻觉之间的相互作用提供了新的见解。我们实验的代码和数据可在https://github.com/ZicongHe2002/HCL - Spark获取。 |
|**2025-03-04**|**Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs**|Yuzhe Gu et.al.|[2503.02846](http://arxiv.org/abs/2503.02846)|null|大型语言模型（LLM）在作为各领域的人工智能助手时会出现幻觉现象（即生成不实或无意义的信息）。由于大语言模型的回复中幻觉内容总是与真实内容混杂在一起，以往基于回复层面进行偏好学习的事实性对齐方法在训练过程中不可避免地引入了噪声。因此，本文提出了一种基于直接偏好优化（DPO）的细粒度事实性对齐方法，称为Mask - DPO。Mask - DPO将句子级别的事实性作为掩码信号，仅从偏好样本中事实正确的句子进行学习，并避免对非偏好样本中的事实内容进行惩罚，从而解决了偏好学习中的模糊性问题。大量实验结果表明，尽管训练过程中未见过这些问题及其对应的主题，但Mask - DPO可以显著提高大语言模型对领域内和领域外数据集问题回复的事实性。仅在ANAH训练集上进行训练，Llama3.1 - 8B - Instruct在ANAH测试集上的得分从49.19%提高到77.53%，甚至超过了Llama3.1 - 70B - Instruct的得分（53.44%），同时它在领域外传记数据集上的事实得分也从30.29%提高到39.39%。我们进一步使用不同的训练样本缩放策略研究了Mask - DPO的泛化特性，发现缩放数据集中主题的数量比问题的数量更有效。我们提出了一个关于事实性对齐对大语言模型作用的假设，并基于这一现象的启示进行了概念验证实验以验证该假设。我们希望该方法和研究结果能为未来的事实性对齐扩展研究铺平道路。 |
|**2025-03-04**|**AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation**|Songming Zhang et.al.|[2503.02832](http://arxiv.org/abs/2503.02832)|null|在现代大语言模型（LLM）中，大语言模型对齐至关重要，通常通过人类反馈强化学习（RLHF）和直接偏好优化（DPO）等方法实现。然而，在大多数现有的大语言模型对齐方法中，回复中的所有标记都使用稀疏的、基于回复级别的奖励或偏好注释进行优化。忽略标记级别的奖励可能会错误地惩罚高质量标记或鼓励低质量标记，导致性能欠佳和收敛速度缓慢。为解决这一问题，我们提出了AlignDistil，这是一种用于标记级奖励优化的与人类反馈强化学习等效的蒸馏方法。具体而言，我们将直接偏好优化学习到的奖励引入人类反馈强化学习目标中，并从理论上证明了该目标与标记级蒸馏过程的等效性，其中教师分布是直接偏好优化模型和参考模型的对数概率的线性组合。在此基础上，我们通过构建一个包含正向和反向直接偏好优化模型的对比式直接偏好优化奖励，进一步缩小了直接偏好优化模型的奖励与纯奖励模型之间的准确性差距。此外，为避免对不同标记进行欠优化和过优化，我们设计了一种标记自适应对数概率外推机制，为每个标记构建合适的教师分布。实验结果表明，我们的AlignDistil方法优于现有方法，并且由于其标记级别的分布式奖励优化，展现出了快速收敛的特性。 |
|**2025-03-04**|**RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration**|Alicia Russell-Gilbert et.al.|[2503.02800](http://arxiv.org/abs/2503.02800)|null|复杂工业环境中的异常检测面临着独特的挑战，尤其是在数据稀疏和运行条件不断变化的情况下。在此类环境中，预测性维护（PdM）需要具备适应性、可迁移性且能够整合特定领域知识的方法。在本文中，我们提出了RAAD - LLM，这是一种用于自适应异常检测的新型框架，它利用了与检索增强生成（RAG）相结合的大语言模型（LLM）。该方法解决了上述PdM挑战。通过有效利用特定领域知识，RAAD - LLM无需在特定数据集上进行微调，即可提高对时间序列数据中异常的检测能力。该框架的自适应机制使其能够动态调整对正常运行条件的理解，从而提高检测准确性。我们通过塑料制造厂的实际应用和斯科尔科沃科技学院异常基准测试（SKAB）验证了这一方法。结果显示，与我们之前的模型相比有显著改进，在实际数据集上的准确率从70.7提高到了89.1。通过允许用语义丰富输入序列数据，RAAD - LLM具备了多模态能力，有助于模型与工厂操作员之间进行更协作的决策。总体而言，我们的研究结果表明RAAD - LLM有能力革新PdM中的异常检测方法，有可能引发各行业异常检测实施方式的范式转变。 |

<p align=right>(<a href=#updated-on-20250305>back to top</a>)</p>

